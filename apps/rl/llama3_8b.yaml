# Config for GRPO finetuning using a Llama3.1 8B Instruct model
#
# This config assumes that you've run the following command before launching
# this run:
#   export HF_HUB_DISABLE_XET=1
#   uv run forge download meta-llama/Meta-Llama-3.1-8B-Instruct


trainer:
  comm:
    trace_buf_size: 0

  model:
    name: llama3
    flavor: 8B
    tokenizer_path: /tmp/Meta-Llama-3.1-8B-Instruct

  processes:
    scheduler: local # local | mast (not supported yet)
    num_hosts: 1
    num_procs: 4

  optimizer:
    name: AdamW
    lr: 1e-5
    eps: 1e-8

  lr_scheduler:
    warmup_steps: 1

  training:
    local_batch_size: 1
    seq_len: 2048
    max_norm: 1.0
    steps: 5
    compile: false
    dataset: "c4"

  parallelism:
    data_parallel_replicate_degree: 1
    data_parallel_shard_degree: -1
    tensor_parallel_degree: 1
    pipeline_parallel_degree: 1
    context_parallel_degree: 1
    expert_parallel_degree: 1
    disable_loss_parallel: false

  checkpoint:
    enable_checkpoint: true
    folder: /tmp/Meta-Llama-3.1-8B-Instruct/saved_checkpoints
    initial_load_path: /tmp/Meta-Llama-3.1-8B-Instruct/
    initial_load_in_hf: true
    last_save_in_hf: true
    interval: 500
    async_mode: "disabled"

  activation_checkpoint:
    mode: selective
    selective_ac_option: op

replay_buffer:
  batch_size: 2
  max_policy_age: 2
  seed: None
  processes:
    scheduler: local # local | mast (not supported yet)
    num_hosts: 1
    num_procs: 1

# policy:
#   scheduler:
#     scheduler: local # local | mast (not supported yet)
#     num_hosts: 1
#     num_gpus: 1
#     oncall: torchtune
#     identity: pytorch_distributed
#     image: forge_workspace:latest
#
#   model: "meta-llama/Llama-3.1-8B-Instruct"
#   tensor_parallel_size: 2
#   pipeline_parallel_size: 1
#   enforce_eager: false

# postprocessor:
#   scheduler:
#     scheduler: local # local | mast (not supported yet)
#     num_hosts: 1
#     num_gpus: 1
#     oncall: torchtune
#     identity: pytorch_distributed
#     image: forge_workspace:latest
#
#   comm:
#     trace_buf_size: 0
#
#   optimizer:
#     name: AdamW
#     lr: 1e-5
#     eps: 1e-8
#
#   lr_scheduler:
#     warmup_steps: 1
#
#   training:
#     local_batch_size: 1
#     seq_len: 2048
#     max_norm: 1.0
#     steps: 5
#     compile: false
#     dataset: "c4"
#
#   parallelism:
#     data_parallel_replicate_degree: 1
#     data_parallel_shard_degree: -1
#     tensor_parallel_degree: 1
#     pipeline_parallel_degree: 1
#     context_parallel_degree: 1
#     expert_parallel_degree: 1
#     disable_loss_parallel: false
#
#   checkpoint:
#     enable_checkpoint: true
#     folder: /tmp/Meta-Llama-3.1-8B-Instruct/
#     interval: 500
#     async_mode: "disabled"
#
#   activation_checkpoint:
#     mode: selective
#     selective_ac_option: op
#

# profiling:
#   enable_profiling: false

# metrics:
#   log_freq: 10
#   enable_tensorboard: true
#   save_tb_folder: "tb"
